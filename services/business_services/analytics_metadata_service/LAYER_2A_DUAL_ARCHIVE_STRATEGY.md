# Layer 2a: Dual Archive Strategy

**Date**: November 8, 2025  
**Purpose**: Document the dual archival strategy for complete data lineage

---

## ğŸ¯ Overview

**Layer 2a (Archive Layer)** stores TWO types of archived data:

1. **Analytics Data Archive** - Transformed data from Layer 2
2. **Source Data Archive** - Raw data from Layer 3 â­ **NEW**

---

## ğŸ“Š Dual Archive Structure

```
Azure Data Lake Storage Gen2
Container: timescaledb-archive

/analytics_service/
  â”œâ”€â”€ analytics/              â† Transformed data from Layer 2
  â”‚   â”œâ”€â”€ customers/
  â”‚   â”‚   â””â”€â”€ 2024/
  â”‚   â”‚       â””â”€â”€ 11/
  â”‚   â”‚           â””â”€â”€ 01/
  â”‚   â”‚               â””â”€â”€ chunk_12345.parquet
  â”‚   â”œâ”€â”€ scor_metrics/
  â”‚   â”‚   â””â”€â”€ 2024/
  â”‚   â”‚       â””â”€â”€ 10/
  â”‚   â”‚           â””â”€â”€ 15/
  â”‚   â”‚               â””â”€â”€ chunk_67890.parquet
  â”‚   â””â”€â”€ orders/
  â”‚       â””â”€â”€ 2024/...
  â”‚
  â””â”€â”€ source/                 â† Raw data from Layer 3 (NEW!)
      â”œâ”€â”€ customers/
      â”‚   â””â”€â”€ 2024/
      â”‚       â””â”€â”€ 11/
      â”‚           â””â”€â”€ 08/
      â”‚               â”œâ”€â”€ source_batch_001.parquet
      â”‚               â””â”€â”€ source_batch_002.parquet
      â”œâ”€â”€ scor_metrics/
      â”‚   â””â”€â”€ 2024/
      â”‚       â””â”€â”€ 11/
      â”‚           â””â”€â”€ 08/
      â”‚               â””â”€â”€ source_batch_003.parquet
      â””â”€â”€ orders/
          â””â”€â”€ 2024/...
```

---

## ğŸ”„ Data Flow with Dual Archival

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ External System (CRM, ERP, Old SCOR Service)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Integration API
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: source_customers (TimescaleDB)                    â”‚
â”‚ - Raw data as received                                      â”‚
â”‚ - Temporary staging (7 days)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â”‚
         â”‚ ETL/Transform                      â”‚ Archive Source
         â”‚                                    â”‚ (Immediate)
         â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: customers      â”‚    â”‚ LAYER 2a: source/customers/  â”‚
â”‚ - Transformed data      â”‚    â”‚ - Original raw data          â”‚
â”‚ - Active (90 days)      â”‚    â”‚ - Lineage preservation       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ - Audit trail                â”‚
         â”‚                     â”‚ - Retained 7+ years          â”‚
         â”‚ Age out             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ (90 days)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2a: analytics/customers/                              â”‚
â”‚ - Archived transformed data                                 â”‚
â”‚ - Historical analytics                                      â”‚
â”‚ - Retained 7+ years                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Why Archive Layer 3 Source Data?

### 1. **Complete Lineage Transparency** ğŸ”
- Track data from original source to final analytics
- Prove data authenticity for audits
- Document transformation history

### 2. **Audit Compliance** ğŸ”’
- SOX, GDPR, HIPAA requirements
- Immutable source records
- Regulatory reporting

### 3. **Recoverability** ğŸ”„
- Replay transformations if bugs found
- Rebuild analytics from source
- Disaster recovery capability

### 4. **Data Quality Forensics** ğŸ”¬
- Investigate discrepancies
- Compare source vs transformed
- Root cause analysis

### 5. **Transformation Validation** âœ…
- Verify ETL logic correctness
- Test new transformation rules
- A/B test different mappings

---

## ğŸ“‹ Archival Policies

### Analytics Data (Layer 2 â†’ Layer 2a)
```python
{
    "source_layer": "layer_2",
    "archive_type": "analytics",
    "table_name": "scor_metrics",
    "retention_in_layer_2": "90 days",
    "archive_after": "90 days",
    "archive_retention": "7 years",
    "archive_format": "parquet",
    "compression": "snappy",
    "partition_by": "day"
}
```

### Source Data (Layer 3 â†’ Layer 2a) â­ NEW
```python
{
    "source_layer": "layer_3",
    "archive_type": "source",
    "table_name": "source_scor_metrics",
    "retention_in_layer_3": "7 days",
    "archive_after": "immediate",  # Archive as soon as processed
    "archive_retention": "7 years",
    "archive_format": "parquet",
    "compression": "snappy",
    "partition_by": "ingestion_date",
    "preserve_raw_format": true
}
```

---

## ğŸ” Querying Archived Data

### Query Analytics Archive
```python
# Historical analytics data (transformed)
result = archival_service.retrieve_archived_data(
    table_name="scor_metrics",
    data_type="analytics",
    start_time="2024-01-01T00:00:00Z",
    end_time="2024-01-31T23:59:59Z",
    columns=["metric_id", "value", "timestamp", "process_id"]
)
```

### Query Source Archive (Lineage)
```python
# Original source data (raw)
result = archival_service.retrieve_archived_data(
    table_name="scor_metrics",
    data_type="source",
    start_time="2024-11-01T00:00:00Z",
    end_time="2024-11-08T23:59:59Z",
    columns=["raw_id", "raw_data", "ingested_at"]
)
```

### Compare Source vs Analytics
```python
# Lineage analysis - compare original vs transformed
source_data = archival_service.retrieve_archived_data(
    table_name="scor_metrics",
    data_type="source",
    start_time="2024-11-01",
    end_time="2024-11-01"
)

analytics_data = archival_service.retrieve_archived_data(
    table_name="scor_metrics",
    data_type="analytics",
    start_time="2024-11-01",
    end_time="2024-11-01"
)

# Validate transformation
validate_transformation(source_data, analytics_data)
```

---

## ğŸ’¡ Use Cases

### 1. **Audit Investigation**
```
Auditor: "Show me the original data received on 2024-03-15"
â†’ Query Layer 2a source archive
â†’ Retrieve raw data as it was received
â†’ Prove data authenticity
```

### 2. **Transformation Bug Fix**
```
Developer: "We found a bug in the ETL logic from last month"
â†’ Query Layer 2a source archive for affected period
â†’ Fix transformation logic
â†’ Replay transformation on archived source data
â†’ Update Layer 2 analytics data
```

### 3. **Data Quality Investigation**
```
Analyst: "Why is this metric showing unexpected values?"
â†’ Query Layer 2a analytics archive (transformed data)
â†’ Query Layer 2a source archive (raw data)
â†’ Compare source vs transformed
â†’ Identify transformation issue or source data problem
```

### 4. **Compliance Reporting**
```
Compliance Officer: "Prove data lineage for regulatory audit"
â†’ Show source data in Layer 2a source archive
â†’ Show transformation logic in code
â†’ Show analytics data in Layer 2a analytics archive
â†’ Complete end-to-end lineage documented
```

### 5. **Disaster Recovery**
```
DBA: "Layer 2 database corrupted, need to rebuild"
â†’ Query Layer 2a source archive for all raw data
â†’ Replay all transformations
â†’ Rebuild Layer 2 analytics tables
â†’ Full recovery from source
```

---

## ğŸ“Š Storage Comparison

| Aspect | Analytics Archive | Source Archive |
|--------|------------------|----------------|
| **Source** | Layer 2 (transformed) | Layer 3 (raw) |
| **Format** | Parquet (columnar) | Parquet (preserves raw structure) |
| **Purpose** | Historical analytics | Lineage & audit |
| **Partition** | By timestamp | By ingestion_date |
| **Compression** | High (snappy) | High (snappy) |
| **Query Pattern** | Trend analysis | Forensic analysis |
| **Retention** | 7+ years | 7+ years |
| **Size** | Larger (more columns) | Smaller (raw only) |
| **Update** | Immutable | Immutable |

---

## ğŸ”‘ Key Benefits

### Cost Optimization
- âœ… Layer 3 only keeps 7 days (minimal database storage)
- âœ… Source data archived to cheap object storage
- âœ… 90% cost reduction vs keeping in TimescaleDB

### Complete Lineage
- âœ… Source data preserved forever
- âœ… Transformation history documented
- âœ… End-to-end traceability

### Compliance & Audit
- âœ… Immutable source records
- âœ… Regulatory compliance (SOX, GDPR, HIPAA)
- âœ… Audit trail from source to analytics

### Recoverability
- âœ… Replay transformations
- âœ… Disaster recovery
- âœ… Bug fixes without data loss

### Data Quality
- âœ… Root cause analysis
- âœ… Transformation validation
- âœ… Source vs analytics comparison

---

## ğŸš€ Implementation

### Archival Service Configuration
```python
# config.py
ARCHIVAL_POLICIES = [
    {
        "name": "analytics_data_archival",
        "source_layer": "layer_2",
        "archive_type": "analytics",
        "retention_days": 90,
        "archive_format": "parquet"
    },
    {
        "name": "source_data_archival",  # NEW!
        "source_layer": "layer_3",
        "archive_type": "source",
        "retention_days": 7,
        "archive_immediately": true,  # Archive as soon as processed
        "archive_format": "parquet",
        "preserve_raw_format": true
    }
]
```

### ETL Process with Dual Archival
```python
async def process_source_data(source_record):
    """Process source data with dual archival."""
    
    # 1. Store in Layer 3 (temporary)
    await db.execute(
        "INSERT INTO source_customers VALUES (...)"
    )
    
    # 2. Archive source data to Layer 2a (immediate)
    await archival_service.archive_source_data(
        table_name="source_customers",
        data=source_record,
        archive_type="source"
    )
    
    # 3. Transform to Layer 2 format
    transformed = transform_customer_data(source_record)
    
    # 4. Insert into Layer 2 (analytics)
    await db.execute(
        "INSERT INTO customers VALUES (...)"
    )
    
    # 5. Mark source as processed
    await db.execute(
        "UPDATE source_customers SET processed = true WHERE id = ..."
    )
    
    # Note: Layer 2 data will be archived to Layer 2a after 90 days
```

---

## ğŸ¯ Summary

**Dual Archive Strategy provides:**

1. **Analytics Archive** (Layer 2 â†’ Layer 2a)
   - Historical analytics data
   - Trend analysis
   - Performance metrics over time

2. **Source Archive** (Layer 3 â†’ Layer 2a) â­ **NEW**
   - Original raw data
   - Complete lineage
   - Audit compliance
   - Transformation recoverability

**Result**: Enterprise-grade data platform with:
- ğŸ’° Cost optimization
- ğŸ” Complete transparency
- ğŸ”’ Regulatory compliance
- ğŸ”„ Full recoverability
- ğŸ“Š Historical analytics

**Both archives accessible via archival service API for complete data lifecycle management!** ğŸš€
