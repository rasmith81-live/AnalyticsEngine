# Complete Data Architecture: Knowledge Graph & Multi-Layer Storage

**Date**: December 8, 2025  
**Status**: Comprehensive architecture with Knowledge Graph ontology

---

## ğŸ—ï¸ Architecture Overview

The analytics platform uses a **Knowledge Graph-based metadata layer** combined with a **four-layer data storage architecture** that separates metadata, active analytics data, archived data, and source data into distinct layers with clear responsibilities.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 0: KNOWLEDGE GRAPH (Business Metadata)                   â”‚
â”‚ â”œâ”€ Ontology: Entities, Metrics, Relationships, Dimensions      â”‚
â”‚ â”œâ”€ Business: Companies, Actors, Strategies, Use Cases          â”‚
â”‚ â”œâ”€ Authorization: Clients, Roles, Permissions, Security        â”‚
â”‚ â”œâ”€ Geography: Countries, Regions, MSAs, Industries             â”‚
â”‚ â”œâ”€ External: Events, News, Market Conditions                   â”‚
â”‚ â”œâ”€ Location: TimescaleDB (business_metadata schema)            â”‚
â”‚ â””â”€ Purpose: Unified ontology for all business concepts         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼ CQRS Scripts (table creation from ontology)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: ANALYTICS MODEL (Active Business Data)                â”‚
â”‚ â”œâ”€ Customers, Orders, Products, SCOR Processes, etc.           â”‚
â”‚ â”œâ”€ Location: TimescaleDB (analytics_data schema)               â”‚
â”‚ â”œâ”€ Format: PostgreSQL tables + TimescaleDB hypertables         â”‚
â”‚ â””â”€ Purpose: Active data for real-time analytics                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼ Archival Service (aging data)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2a: ARCHIVE (Historical Data Lake)                       â”‚
â”‚ â”œâ”€ Archived chunks from Layer 2 tables                         â”‚
â”‚ â”œâ”€ Location: Azure Data Lake Storage Gen2                      â”‚
â”‚ â”œâ”€ Format: Parquet/Delta Lake (columnar storage)               â”‚
â”‚ â””â”€ Purpose: Long-term storage, historical analytics            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚ ETL/Mapping
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: SOURCE MODEL (Integration/Staging)                    â”‚
â”‚ â”œâ”€ Raw data from external systems                              â”‚
â”‚ â”œâ”€ Location: TimescaleDB (integration_data schema)             â”‚
â”‚ â”œâ”€ Format: PostgreSQL staging tables                           â”‚
â”‚ â””â”€ Purpose: Temporary staging for data transformation          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚ Integration Services
                    External Systems (CRM, ERP, etc.)
```

---

## ğŸ“Š Layer 0: Knowledge Graph (Business Metadata Layer)

**Purpose**: Store the **unified ontology** that describes all business concepts, relationships, and metadata

**Service**: `business_metadata` at `services/business_services/business_metadata`

**Database**: TimescaleDB schema `business_metadata`

**Architecture**: Knowledge Graph with versioned, relationship-rich metadata

### What's Stored

The Knowledge Graph ontology contains **26 primary definition classes** organized into 5 layers:

*Note: Core layer contains 6 classes, Business layer contains 8 classes*

#### **Core Ontology Layer**
- `ThingDefinition` - Base class for all definitions
- `EntityDefinition` - Business entity definitions (customers, orders, products)
- `RelationshipDefinition` - Relationships between entities
- `MetricDefinition` - KPI and metric definitions with formulas
- `ValueSetDefinition` - Enumerated value sets
- `CodeSystemDefinition` - Code system definitions

#### **Business Ontology Layer**
- `ValueChainPatternDefinition` - Value chain patterns (base class)
- `ActorDefinition` - Actors (people, roles, organizations, systems)
- `BeneficiaryDefinition` - Value chain beneficiaries
- `CompanyDefinition` - Company-level value chains (inherits from ValueChainPattern)
- `BusinessProcessDefinition` - Process-level value chains (inherits from ValueChainPattern)
- `StrategicObjectiveDefinition` - Strategic business objectives
- `BenchmarkDefinition` - Industry benchmarks with citations
- `ExternalEventDefinition` - External events/news impacting business

#### **Authorization & Access Control Layer**
- `ClientDefinition` - Multi-tenant clients/organizations
- `RoleDefinition` - Roles within client organizations
- `PermissionDefinition` - Base permission class
- `ModulePermissionDefinition` - Module-level access control
- `EntityPermissionDefinition` - Entity-level access control
- `MetricPermissionDefinition` - Metric-level access control
- `AttributePermissionDefinition` - Attribute-level access with masking
- `RowLevelSecurityDefinition` - Row-level security filters

#### **Geographic & Industry Classification Layer**
- `CountryDefinition` - Countries with ISO 3166-1 codes
- `RegionDefinition` - States, provinces, territories
- `MetropolitanAreaDefinition` - MSAs and CMAs
- `NAICSIndustryDefinition` - NAICS industry codes

#### **Analytics Strategy & Data Management Layer**
- `AnalyticsStrategyDefinition` - Company analytics strategy and maturity
- `DataSourceDefinition` - Data sources with quality tracking
- `DataProductDefinition` - Curated data assets with SLAs
- `AnalyticsUseCaseDefinition` - Business problems solved with analytics
- `DimensionDefinition` - Analytical dimensions for slicing
- `MetricCategoryDefinition` - Hierarchical metric categorization
- `DataQualityRuleDefinition` - Data quality validation rules

### Key Characteristics

- âœ… **Unified Ontology** - Single source of truth for all business concepts
- âœ… **Graph-Based** - Rich relationships between all entities
- âœ… **Versioned** - Full version history and temporal queries
- âœ… **JSONB Storage** - Flexible schema with PostgreSQL JSONB
- âœ… **Pydantic Models** - Type-safe Python models with validation
- âœ… **RESTful API** - FastAPI endpoints for CRUD operations
- âœ… **Cached** - Redis caching for performance
- âœ… **Searchable** - Full-text search and filtering

### Storage Structure

```
services/business_services/business_metadata/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ metadata_api.py          â† RESTful API endpoints
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ metadata_service.py      â† High-level orchestration
â”‚   â””â”€â”€ metadata_instantiation_service.py  â† Model conversion
â”œâ”€â”€ repositories/
â”‚   â””â”€â”€ metadata_repository.py   â† Database access layer
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ metadata_definition.py   â† SQLAlchemy models
â”‚   â””â”€â”€ metadata_version.py      â† Version tracking
â””â”€â”€ main.py                      â† FastAPI application

services/business_services/analytics_metadata_service/definitions/
â””â”€â”€ ontology_models.py           â† Pydantic ontology definitions
```

### Database Schema

```sql
-- Core metadata table with JSONB storage
CREATE TABLE business_metadata.metadata_definitions (
    id UUID PRIMARY KEY,
    kind VARCHAR(100) NOT NULL,
    code VARCHAR(100),
    name VARCHAR(255) NOT NULL,
    description TEXT,
    definition_data JSONB NOT NULL,  -- Full Pydantic model as JSON
    version INTEGER NOT NULL,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    created_by VARCHAR(100),
    updated_by VARCHAR(100),
    metadata_ JSONB DEFAULT '{}'::jsonb
);

-- Version history table
CREATE TABLE business_metadata.metadata_versions (
    id UUID PRIMARY KEY,
    definition_id UUID REFERENCES metadata_definitions(id),
    version INTEGER NOT NULL,
    definition_data JSONB NOT NULL,
    change_description TEXT,
    changed_at TIMESTAMPTZ DEFAULT NOW(),
    changed_by VARCHAR(100)
);

-- Indexes for performance
CREATE INDEX idx_metadata_kind ON metadata_definitions(kind);
CREATE INDEX idx_metadata_code ON metadata_definitions(code);
CREATE INDEX idx_metadata_active ON metadata_definitions(is_active);
CREATE INDEX idx_metadata_data_gin ON metadata_definitions USING gin(definition_data);
```

### API Endpoints

```
# Core CRUD operations
POST   /api/v1/metadata/definitions          - Create definition
GET    /api/v1/metadata/definitions/{id}     - Get by ID
PUT    /api/v1/metadata/definitions/{id}     - Update definition
DELETE /api/v1/metadata/definitions/{id}     - Delete definition

# Search and query
GET    /api/v1/metadata/definitions          - List all (with filters)
POST   /api/v1/metadata/search               - Advanced search
GET    /api/v1/metadata/kinds                - List all kinds

# Relationships
GET    /api/v1/metadata/definitions/{id}/relationships  - Get relationships
POST   /api/v1/metadata/relationships        - Create relationship

# Versioning
GET    /api/v1/metadata/definitions/{id}/versions       - Version history
GET    /api/v1/metadata/definitions/{id}/versions/{v}   - Get specific version

# Type-specific convenience endpoints
GET    /api/v1/metadata/entities/{code}              - Get entity
GET    /api/v1/metadata/metrics/{code}               - Get metric
GET    /api/v1/metadata/companies/{code}             - Get company
GET    /api/v1/metadata/clients/{code}               - Get client
GET    /api/v1/metadata/analytics-strategies/{code}  - Get strategy
GET    /api/v1/metadata/data-sources/{code}          - Get data source
GET    /api/v1/metadata/external-events/{code}       - Get external event
# ... and 18 more type-specific endpoints
```

### Complete Traceability Chain

The Knowledge Graph enables end-to-end traceability:

```
AnalyticsStrategy (maturity, governance)
  â†“ strategic_priorities
StrategicObjectives (business goals)
  â†“ aligned_use_cases
AnalyticsUseCases (problems to solve)
  â†“ required_data_sources, required_metrics
DataProducts (curated assets)
  â†“ included_metrics, source_entities
Metrics (KPIs)
  â†“ required_objects, data_sources
Entities (data models)
  â†“ table_schema
DataSources (raw data)

PLUS External Context:
ExternalEvents â†’ affected_metrics, affected_entities, related_companies
```

---

## ğŸ“ˆ Layer 2: Analytics Model Layer (Active Business Data)

**Purpose**: Store **active business data** for real-time analytics and reporting

**Database**: TimescaleDB schema `analytics_data`

**What's Stored**:
- **Customer data** - Active customer records
- **Order data** - Recent order records
- **Product data** - Current product catalog
- **SCOR Process data** - Active SCOR process instances
- **SCOR Metric data** - Recent metric measurements
- **All other business entities** - Hot/active operational data

**Key Characteristics**:
- âœ… **Active data** - Recent, frequently accessed records
- âœ… **Dynamically created** - Tables generated from Layer 0 ontology
- âœ… **CQRS pattern** - Write models + Read models
- âœ… **TimescaleDB hypertables** - For time-series data with automatic partitioning
- âœ… **Analytics-ready** - Optimized for querying and reporting
- âœ… **Hot storage** - Fast access, higher cost

**Data Retention**:
- Recent data (e.g., last 90 days for high-frequency data)
- Actively queried records
- Real-time analytics workloads

**How Tables Are Created**:
```
Layer 0 Ontology (EntityDefinition) â†’ CQRS Scripts â†’ Layer 2 Tables
```

**Example Tables**:
```sql
-- CQRS Write Models
analytics_data.customers
analytics_data.scor_processes
analytics_data.scor_metrics

-- CQRS Read Models (optimized for queries)
analytics_data.customers_read
analytics_data.scor_processes_read
analytics_data.scor_metrics_read
```

---

## ğŸ—„ï¸ Layer 2a: Archive Layer (Historical Data Lake)

**Purpose**: Store **historical data** archived from Layer 2 for long-term retention and cost optimization

**Storage**: Azure Data Lake Storage Gen2 (ADLS Gen2)

**Service**: `archival_service` at `services/backend_services/archival_service`

**What's Stored**:
- **Archived TimescaleDB chunks** - Old data moved from Layer 2
- **Historical time-series data** - Aged-out metrics and measurements
- **Compliance data** - Long-term retention for regulatory requirements
- **Historical analytics** - Data for trend analysis over years

**Key Characteristics**:
- âœ… **Cold storage** - Infrequently accessed, low cost
- âœ… **Columnar format** - Parquet or Delta Lake for efficient storage
- âœ… **Compressed** - Optimized for storage efficiency
- âœ… **Queryable** - Can be queried via archival service API
- âœ… **Immutable** - Write-once, read-many pattern
- âœ… **Partitioned** - Organized by date hierarchy (year/month/day)

### Dual Archive Strategy

**Layer 2a contains TWO types of archived data:**

1. **Analytics Data Archive** (from Layer 2)
   - Transformed business data
   - Historical analytics queries
   - Performance metrics over time
   - Trend analysis

2. **Source Data Archive** (from Layer 3)
   - Original raw data as received
   - Preserves data lineage
   - Enables transformation replay
   - Audit trail for compliance

### Storage Format

```
Container: timescaledb-archive

/analytics_data/
  â”œâ”€â”€ analytics/              â† Transformed data from Layer 2
  â”‚   â”œâ”€â”€ customers/
  â”‚   â”‚   â””â”€â”€ 2024/
  â”‚   â”‚       â””â”€â”€ 11/
  â”‚   â”‚           â””â”€â”€ 01/
  â”‚   â”‚               â””â”€â”€ chunk_12345.parquet
  â”‚   â”œâ”€â”€ scor_metrics/
  â”‚   â”‚   â””â”€â”€ 2024/
  â”‚   â”‚       â””â”€â”€ 10/
  â”‚   â”‚           â””â”€â”€ 15/
  â”‚   â”‚               â””â”€â”€ chunk_67890.parquet
  â”‚   â””â”€â”€ orders/
  â”‚       â””â”€â”€ 2024/...
  â”‚
  â””â”€â”€ source/                 â† Raw data from Layer 3
      â”œâ”€â”€ customers/
      â”‚   â””â”€â”€ 2024/
      â”‚       â””â”€â”€ 11/
      â”‚           â””â”€â”€ 08/
      â”‚               â”œâ”€â”€ source_batch_001.parquet
      â”‚               â””â”€â”€ source_batch_002.parquet
      â”œâ”€â”€ scor_metrics/
      â”‚   â””â”€â”€ 2024/
      â”‚       â””â”€â”€ 11/
      â”‚           â””â”€â”€ 08/
      â”‚               â””â”€â”€ source_batch_003.parquet
      â””â”€â”€ orders/
          â””â”€â”€ 2024/...
```

### Archival Process

1. **TimescaleDB chunk aging** - Chunks older than retention policy
2. **Archival event** - Published to Redis queue
3. **Archival service** - Extracts chunk data from TimescaleDB
4. **Data transformation** - Converts to Parquet/Delta format
5. **Upload to ADLS** - Writes to Azure Data Lake Storage
6. **Confirmation** - Publishes success event
7. **Chunk deletion** - TimescaleDB chunk can be dropped (optional)

### Retrieval Process

```python
# Query historical transformed data
archival_service.retrieve_archived_data(
    table_name="scor_metrics",
    data_type="analytics",  # or "source" for raw data
    start_time="2024-01-01",
    end_time="2024-01-31",
    columns=["metric_id", "value", "timestamp"]
)

# Query original source data for lineage/audit
archival_service.retrieve_archived_data(
    table_name="scor_metrics",
    data_type="source",  # Raw source data
    start_time="2024-11-01",
    end_time="2024-11-08"
)
```

---

## ğŸ”„ Layer 3: Source Model Layer (Integration/Staging)

**Purpose**: Store **raw data ingested from source systems** before transformation

**Database**: TimescaleDB schema `integration_data`

**What's Stored**:
- **Source customer data** - Raw data from CRM systems
- **Source order data** - Raw data from ERP systems
- **Source product data** - Raw data from product catalogs
- **Source SCOR data** - Raw data from old SCOR service
- **All other source data** - Unprocessed integration data

**Key Characteristics**:
- âœ… **Raw/staging data** - As received from source systems
- âœ… **Temporary in Layer 3** - Data transformed and moved to Layer 2
- âœ… **Archived to Layer 2a** - Raw data preserved for lineage and audit
- âœ… **Integration layer** - Handles data ingestion
- âœ… **Real-time processing** - Continuous data flow (not batch)
- âœ… **Mapping/transformation** - ETL logic to Layer 2

**Example Tables**:
```sql
-- Staging tables (temporary in TimescaleDB)
integration_data.source_customers
integration_data.source_orders
integration_data.source_scor_processes
```

**Data Flow**:
```
External System â†’ Integration Service â†’ Layer 3 (staging) â†’ 
Transformation/Mapping â†’ Layer 2 (analytics)
       â†“
Layer 2a (archive source data for lineage/audit)
```

**Layer 3 Retention Policy**:
```python
{
    "layer": "source",
    "retention_in_timescaledb": "7 days",  # Keep in Layer 3 for 1 week
    "archive_to_layer_2a": true,           # Archive raw source data
    "archive_retention": "7 years",        # Keep in archive for compliance
    "archive_format": "parquet",           # Compressed columnar format
    "partition_by": "ingestion_date"       # Partition by when data arrived
}
```

---

## ğŸ”„ Complete Data Flow

### End-to-End Journey of a Data Record

```
1. ONTOLOGY DEFINITION (Layer 0)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Knowledge Graph:         â”‚
   â”‚ EntityDefinition         â”‚
   â”‚ - code: "CUSTOMER"       â”‚
   â”‚ - table_schema: {...}    â”‚
   â”‚ - relationships: [...]   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ CQRS Scripts
            â–¼

2. INGESTION (External â†’ Layer 3)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ CRM System  â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚ API/Integration
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Layer 3: source_    â”‚
   â”‚ customers (staging) â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                 â”‚
          â”‚ ETL/Mapping                     â”‚ Archive Source
          â–¼                                 â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Layer 2: customers  â”‚         â”‚ Layer 2a: source/    â”‚
   â”‚ (active analytics)  â”‚         â”‚ customers/2024/12/08/â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ raw_data.parquet     â”‚
                                    â”‚ (source lineage)     â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. ANALYTICS (Layer 2 queries)
          â”‚ Real-time queries
          â”‚ KPI calculations (using MetricDefinition from Layer 0)
          â”‚ Dashboards
          â–¼
   [Active for 90 days]

4. ARCHIVAL (Layer 2 â†’ Layer 2a)
          â”‚ Aging policy
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Layer 2a: analytics/    â”‚
   â”‚ customers/2024/12/01/   â”‚
   â”‚ chunk.parquet           â”‚
   â”‚ (transformed data)      â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. HISTORICAL ANALYTICS (Layer 2a queries)
          â”‚ Historical trends
          â”‚ Compliance reports
          â”‚ Long-term analysis
          â”‚ Data lineage tracking
          â–¼
   [Both source & transformed data retained for 7+ years]
```

---

## ğŸ“Š Layer Comparison Matrix

| Aspect | Layer 0 | Layer 2 | Layer 2a | Layer 3 |
|--------|---------|---------|----------|---------|
| **Purpose** | Ontology & Metadata | Active analytics | Historical archive | Staging |
| **Data Type** | Definitions & Relationships | Business records | Source + Analytics archives | Raw data |
| **Storage** | TimescaleDB | TimescaleDB | Azure Data Lake | TimescaleDB |
| **Schema** | business_metadata | analytics_data | N/A | integration_data |
| **Format** | JSONB | PostgreSQL + Hypertables | Parquet/Delta | PostgreSQL |
| **Access Pattern** | Frequent | Frequent | Occasional | Temporary |
| **Query Speed** | Fast | Fast | Moderate | Fast |
| **Data Volume** | Small (MB-GB) | Medium (GB-TB) | Large (TB-PB) | Small (GB) |
| **Retention** | Permanent (versioned) | 30-90 days | 7+ years | 7 days |
| **Archived to 2a** | No | Yes | N/A | Yes (for lineage) |
| **Cost** | Low | Medium-High | Very Low | Low |
| **CQRS** | No | Yes | No | No |
| **Compression** | No | Minimal | High | No |
| **Versioning** | Yes (full history) | No | No | No |
| **Graph-based** | Yes | No | No | No |

---

## ğŸ¯ Complete Example: Customer Analytics

### Layer 0: Customer Ontology Definition

```python
# EntityDefinition in Knowledge Graph
CUSTOMER_ENTITY = EntityDefinition(
    kind="entity_definition",
    id="Entity:CUSTOMER",
    code="CUSTOMER",
    name="Customer",
    description="Customer entity with demographics and behavior",
    table_schema=TableSchemaDefinition(
        table_name="customers",
        columns=[
            ColumnDefinition(name="customer_id", type="UUID", primary_key=True),
            ColumnDefinition(name="name", type="String(255)"),
            ColumnDefinition(name="email", type="String(255)"),
            ColumnDefinition(name="country_code", type="String(2)"),
            ColumnDefinition(name="created_at", type="DateTime"),
            ColumnDefinition(name="lifetime_value", type="Decimal(10,2)")
        ]
    ),
    relationships=[
        RelationshipDefinition(
            kind="relationship_definition",
            from_entity="CUSTOMER",
            to_entity="ORDER",
            relationship_type="places",
            from_cardinality="1",
            to_cardinality="0..*"
        ),
        RelationshipDefinition(
            kind="relationship_definition",
            from_entity="CUSTOMER",
            to_entity="COUNTRY",
            relationship_type="located_in",
            from_cardinality="*",
            to_cardinality="1"
        )
    ]
)

# MetricDefinition in Knowledge Graph
CUSTOMER_LTV = MetricDefinition(
    kind="metric_definition",
    code="CUSTOMER_LTV",
    name="Customer Lifetime Value",
    description="Total value of customer over their lifetime",
    formula="SUM(Order.total_amount) WHERE Order.customer_id = Customer.customer_id",
    required_objects=["CUSTOMER", "ORDER"],
    unit="USD",
    data_type="decimal",
    aggregation_methods=["sum", "avg", "median"],
    time_periods=["monthly", "quarterly", "yearly"],
    dimensions=["country", "customer_segment", "acquisition_channel"],
    metric_category="FINANCIAL",
    data_sources=["CRM_SYSTEM", "ORDER_SYSTEM"]
)

# AnalyticsUseCaseDefinition in Knowledge Graph
CUSTOMER_SEGMENTATION = AnalyticsUseCaseDefinition(
    kind="analytics_use_case_definition",
    code="CUSTOMER_SEGMENTATION",
    name="Customer Segmentation Analysis",
    use_case_type="reporting",
    business_problem="Identify high-value customer segments for targeted marketing",
    expected_value="20% increase in marketing ROI",
    success_metrics=["CUSTOMER_LTV", "CUSTOMER_RETENTION_RATE"],
    required_data_sources=["CRM_SYSTEM"],
    required_entities=["CUSTOMER", "ORDER"],
    required_metrics=["CUSTOMER_LTV", "CUSTOMER_RETENTION_RATE"],
    business_owner="MARKETING_VP",
    technical_owner="DATA_TEAM_LEAD",
    maturity_stage="production"
)
```

### Layer 2: Active Customer Data (Last 90 Days)

```sql
-- Active customers in TimescaleDB
SELECT * FROM analytics_data.customers
WHERE created_at > NOW() - INTERVAL '90 days';

-- Example: Recent customer
customer_id: "550e8400-e29b-41d4-a716-446655440000"
name: "Acme Corporation"
email: "contact@acme.com"
country_code: "US"
created_at: 2024-12-08 10:00:00
lifetime_value: 125000.00
```

### Layer 2a: Archived Customer Data (Historical)

**Two Archive Types in Layer 2a:**

**1. Analytics Data Archive** (from Layer 2):
```
Azure Data Lake:
/timescaledb-archive/analytics_data/analytics/customers/
  /2024/
    /01/  â† January transformed data
      /15/
        chunk_12345.parquet  (transformed analytics data)
    /02/  â† February transformed data
      /20/
        chunk_12346.parquet
```

**2. Source Data Archive** (from Layer 3):
```
Azure Data Lake:
/timescaledb-archive/analytics_data/source/customers/
  /2024/
    /12/  â† December raw source data
      /08/
        source_batch_001.parquet  (original raw data from CRM)
        source_batch_002.parquet
```

### Layer 3: Source Customer Data (Staging)

```sql
-- Raw data from CRM system (temporary in Layer 3)
INSERT INTO integration_data.source_customers
VALUES (
    raw_id: "crm_12345",
    raw_data: '{"customer_name": "Acme Corp", "email": "contact@acme.com", ...}',
    ingested_at: NOW(),
    processed: false
);

-- Parallel operations:
-- 1. ETL transforms to Layer 2 format
-- 2. Archive raw data to Layer 2a for lineage
-- 3. Mark processed: true
-- 4. Delete from Layer 3 after 7 days
```

---

## ğŸ”‘ Key Relationships Between Layers

### Layer 0 â†’ Layer 2
- **Ontology â†’ Implementation**
- CQRS scripts read `EntityDefinition.table_schema` from Layer 0
- Generate SQLAlchemy models and Alembic migrations
- Create tables in Layer 2
- **MetricDefinition** drives KPI calculation engine

### Layer 2 â†” Layer 2a
- **Active â†” Archive**
- Archival service monitors TimescaleDB chunk age
- Moves old chunks from Layer 2 to Layer 2a
- Can retrieve archived data back for queries
- **Bidirectional** but asymmetric (write to 2a, read from both)

### Layer 3 â†’ Layer 2 (and Layer 2a)
- **Source â†’ Target (with lineage preservation)**
- ETL processes transform raw data
- Map source schema to analytics schema (defined in Layer 0)
- Load into Layer 2 tables
- **Simultaneously archive source data to Layer 2a**
- **Dual flow**: Transform to Layer 2 + Archive to Layer 2a

### Layer 0 â† â†’ Layer 2 â† â†’ Layer 2a
- **Schema consistency**
- Layer 0 defines ontology and schema
- Layer 2 implements schema
- Layer 2a archives data in same schema (but different format)
- Schema changes propagate through all layers

---

## ğŸš€ Benefits of This Architecture

### 1. **Unified Ontology** â­ NEW
- Single source of truth for all business concepts
- Graph-based relationships enable complex queries
- Versioned metadata with full history
- Type-safe Pydantic models with validation
- RESTful API for all metadata operations

### 2. **Complete Traceability**
- AnalyticsStrategy â†’ StrategicObjectives â†’ UseCases â†’ Metrics â†’ Entities â†’ DataSources
- External events linked to affected metrics and entities
- Full data lineage from source to analytics
- Audit trail for compliance

### 3. **Cost Optimization**
- Hot data (Layer 2) on expensive fast storage
- Cold data (Layer 2a) on cheap object storage
- 90% cost reduction for historical data
- Source data archived immediately (no long-term Layer 3 storage costs)

### 4. **Performance**
- Layer 2 stays lean (only recent data)
- Fast queries on active data
- Historical queries still possible via Layer 2a
- Layer 3 stays small (7-day retention only)
- Redis caching for Layer 0 metadata

### 5. **Scalability**
- Layer 2a can store unlimited data
- No database size constraints
- Horizontal scaling via partitioning
- Both source and analytics data scale independently
- Knowledge Graph scales with business complexity

### 6. **Compliance & Audit**
- Long-term retention (7+ years) in Layer 2a
- **Complete data lineage** - Source data preserved
- **Audit trail** - Track transformations from source to analytics
- Regulatory compliance (SOX, GDPR, HIPAA, etc.)
- **Immutable source records** - Original data never modified
- Row-level security and data masking in Layer 0

### 7. **Data Quality & Recoverability**
- **Replay transformations** - Reprocess from archived source data
- **Root cause analysis** - Investigate data quality issues
- **Disaster recovery** - Rebuild analytics from source
- **Transformation validation** - Compare source vs transformed
- **Bug fixes** - Correct transformation logic and reprocess
- Data quality rules defined in Layer 0

### 8. **Flexibility**
- Multiple storage formats (Parquet, Delta)
- Can query across layers
- Time travel with Delta Lake
- **Dual archives** - Source and analytics data separately queryable
- Extensible ontology for new business concepts

### 9. **Business Context**
- Geographic and industry classification
- External events and market conditions
- Analytics strategy and maturity tracking
- Use case management and prioritization
- Benchmark comparisons

---

## ğŸ¯ Current Status

### Layer 0: âœ… COMPLETE
- **26 ontology classes** defined in `ontology_models.py`
- **business_metadata service** deployed with FastAPI
- **RESTful API** with 30+ endpoints
- **Version tracking** and history
- **Redis caching** enabled
- **PostgreSQL JSONB** storage
- Ready to drive Layer 2 creation

### Layer 2: â³ NEXT STEP
- Need to create tables from Layer 0 ontology
- Run CQRS scripts for entity tables
- Apply Alembic migrations
- Implement CQRS read/write models

### Layer 2a: âœ… OPERATIONAL
- Archival service deployed and running
- Azure Data Lake Storage configured
- Parquet/Delta Lake support enabled
- Dual archive strategy (analytics + source)
- Ready to archive Layer 2 data

### Layer 3: ğŸ”® FUTURE
- Will be created when integrations are built
- Handles data ingestion from external systems
- Maps to Layer 2 analytics tables
- Immediate archival to Layer 2a for lineage

---

## ğŸ“‹ Service Locations

```
services/business_services/
â”œâ”€â”€ business_metadata/              â† Layer 0: Knowledge Graph
â”‚   â”œâ”€â”€ api/metadata_api.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ metadata_service.py
â”‚   â”‚   â””â”€â”€ metadata_instantiation_service.py
â”‚   â”œâ”€â”€ repositories/metadata_repository.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ metadata_definition.py
â”‚   â”‚   â””â”€â”€ metadata_version.py
â”‚   â””â”€â”€ main.py
â”‚
â””â”€â”€ analytics_metadata_service/
    â””â”€â”€ definitions/
        â””â”€â”€ ontology_models.py      â† Pydantic ontology definitions

services/backend_services/
â””â”€â”€ archival_service/               â† Layer 2a: Archive
    â”œâ”€â”€ lakehouse_client.py
    â”œâ”€â”€ archival_processor.py
    â”œâ”€â”€ management.py
    â””â”€â”€ tasks.py
```

---

## ğŸ‰ Summary

**Four-layer architecture with Knowledge Graph ontology provides:**

- âœ… **Layer 0** - Knowledge Graph ontology (unified metadata, 26 classes in 5 layers)
  - Core Ontology (6 classes)
  - Business Ontology (8 classes including ValueChainPatternDefinition and ExternalEventDefinition)
  - Authorization & Access Control (8 classes)
  - Geographic & Industry Classification (4 classes)
  - Analytics Strategy & Data Management (7 classes)
- âœ… **Layer 2** - Active analytics data (hot storage, 30-90 days)
- âœ… **Layer 2a** - Archived data (cold storage, 7+ years)
  - Analytics data archive (from Layer 2)
  - Source data archive (from Layer 3) for lineage & audit
- âœ… **Layer 3** - Source/staging data (integration, 7 days)

**Complete data lifecycle with ontology-driven design**:
```
Layer 0 (Knowledge Graph Ontology)
    â†“ defines structure
External System â†’ Layer 3 (staging) â”€â”€â”¬â†’ Layer 2 (active) â†’ Layer 2a (analytics archive)
                  [7 days]            â”‚  [30-90 days]       [7+ years]
                                      â”‚
                                      â””â†’ Layer 2a (source archive)
                                         [7+ years - lineage preservation]
```

**Key Innovations**: 
- **Knowledge Graph ontology** - Unified, versioned, graph-based metadata
- **26 ontology classes** - Complete business, authorization, geography, and strategy
- **Dual archival strategy** - Complete data lineage preservation
- **End-to-end traceability** - From strategy to data sources
- **External context** - News and events impact analysis

**This architecture enables:**
- ğŸ’° Cost-effective storage
- ğŸ“Š Scalable analytics
- ğŸ”’ Regulatory compliance
- ğŸ” Complete data lineage
- ğŸ”„ Transformation recoverability
- ğŸ“ˆ Historical trend analysis
- ğŸ¯ Strategy-driven analytics
- ğŸŒ Geographic and industry segmentation
- ğŸ“° External event impact tracking

**Enterprise-grade data platform with Knowledge Graph foundation!** ğŸš€
